pip install transformers datasets torch   \\(terminal se phela install karligiye ga) 



\\(yeh pura code hai )
from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling
import torch

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')
def load_dataset(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    return "".join(lines)

def save_tokenized_dataset(dataset, tokenizer, output_file):
    tokenized_data = tokenizer(dataset, return_tensors="pt", truncation=True, padding=True)
    torch.save(tokenized_data, output_file)
dataset_path = 'education_dataset.txt'
dataset = load_dataset(dataset_path)
tokenized_output_file = 'tokenized_dataset.pt'

save_tokenized_dataset(dataset, tokenizer, tokenized_output_file)

def load_tokenized_dataset(filepath):
    return torch.load(filepath)

tokenized_dataset = load_tokenized_dataset(tokenized_output_file)

def fine_tune_model(tokenized_dataset, model, tokenizer, output_dir, epochs=3):
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # GPT-2 does not use masked language modeling
    )
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=epochs,
        per_device_train_batch_size=2,  # Adjust this based on your hardware
        save_steps=100,
        save_total_limit=2,
        prediction_loss_only=True
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator
    )

    
    trainer.train()

    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)

output_dir = './education_chatbot'
fine_tune_model(tokenized_dataset, model, tokenizer, output_dir)





\\(code test karna ke liye)
from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained('./education_chatbot')
tokenizer = GPT2Tokenizer.from_pretrained('./education_chatbot')
def generate_response(question, model, tokenizer, max_length=150):
    input_ids = tokenizer.encode(question, return_tensors='pt')
    outputs = model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response
print("Chatbot is ready to assist you. Type your questions below.")
while True:
    user_input = input("You: ")
    if user_input.lower() in ['exit', 'quit']:
        print("Goodbye!")
        break
    response = generate_response(user_input, model, tokenizer)

\\Agar without train karke chahiye tow aoko API use karna hoga chatgpt ya phir dusra models ka tow code thora easy ho jayega, isme apko apna database datas dalna hoga. Jitna jyada datas hoga utna jyada fine tune hoga 
